# Reinforcement Learning - Aprendizaje por Refuerzo

Este directorio contiene recursos y trabajos relacionados con Reinforcement Learning.

## 游닄 Temas Principales

### 1. Fundamentos
- **Conceptos B치sicos**
  - Agent, Environment, State, Action, Reward
  - Policy (): estrategia del agente
  - Value Function: V(s) y Q(s,a)
  - Model-based vs Model-free
  - On-policy vs Off-policy

- **Markov Decision Process (MDP)**
  - Estados, acciones, transiciones
  - Recompensas y retornos
  - Factor de descuento (풥)
  - Bellman Equations

### 2. M칠todos Tabulares
- **Dynamic Programming**
  - Policy Iteration
  - Value Iteration
  - Policy Evaluation

- **Monte Carlo Methods**
  - First-visit MC
  - Every-visit MC
  - MC Control
  - Exploring Starts

- **Temporal-Difference (TD) Learning**
  - TD(0)
  - SARSA (State-Action-Reward-State-Action)
  - Q-Learning
  - Expected SARSA

- **n-step Bootstrapping**
  - n-step TD
  - n-step SARSA
  - n-step Q-Learning

### 3. Value-Based Deep RL
- **Deep Q-Networks (DQN)**
  - Experience Replay
  - Target Networks
  - Variantes:
    - Double DQN
    - Dueling DQN
    - Prioritized Experience Replay
    - Rainbow DQN
    - Noisy DQN

### 4. Policy-Based Methods
- **Policy Gradient**
  - REINFORCE
  - Baseline methods
  - Actor-Critic
  - Advantage function

- **Advanced Policy Gradient**
  - **A3C** (Asynchronous Advantage Actor-Critic)
  - **A2C** (Advantage Actor-Critic)
  - **PPO** (Proximal Policy Optimization)
  - **TRPO** (Trust Region Policy Optimization)

### 5. Actor-Critic Methods
- **Deterministic Policy Gradient**
  - **DDPG** (Deep Deterministic Policy Gradient)
  - **TD3** (Twin Delayed DDPG)

- **Soft Actor-Critic (SAC)**
  - Maximum entropy RL
  - Off-policy learning

### 6. Model-Based RL
- **World Models**
  - Learning environment dynamics
  - Planning in learned models

- **Dyna-Q**
  - Planning and learning
  - Simulated experience

- **AlphaZero/MuZero**
  - Monte Carlo Tree Search (MCTS)
  - Self-play

### 7. Multi-Agent RL
- Cooperative agents
- Competitive agents
- Mixed scenarios
- QMIX, MADDPG

### 8. Inverse RL y Imitation Learning
- Learning from demonstrations
- Behavioral cloning
- Inverse Reinforcement Learning
- GAIL (Generative Adversarial Imitation Learning)

### 9. Hierarchical RL
- Options framework
- Feudal Networks
- Goal-conditioned RL

## 游댢 Herramientas y Bibliotecas

### Entornos de Simulaci칩n
- **OpenAI Gym**: Est치ndar de facto para RL
- **Gymnasium**: Fork mantenido de Gym
- **Unity ML-Agents**: Entornos 3D
- **MuJoCo**: F칤sica para rob칩tica
- **PyBullet**: F칤sica de c칩digo abierto
- **Atari**: Juegos cl치sicos de Atari
- **PettingZoo**: Multi-agent environments

### Bibliotecas de RL
- **Stable-Baselines3**: Implementaciones de algoritmos populares (PyTorch)
- **RLlib (Ray)**: RL escalable y distribuido
- **TF-Agents**: TensorFlow para RL
- **Dopamine**: Framework de Google para investigaci칩n
- **CleanRL**: Implementaciones simples y limpias
- **Spinning Up (OpenAI)**: Recursos educativos

### Herramientas de Visualizaci칩n
- **TensorBoard**: Monitoreo de entrenamiento
- **Weights & Biases**: Tracking de experimentos
- **Gym Monitor**: Grabaci칩n de episodios

## 游닀 Recursos Recomendados

### Cursos
- [CS285 - Deep Reinforcement Learning (UC Berkeley)](http://rail.eecs.berkeley.edu/deeprlcourse/)
- [David Silver's RL Course (DeepMind)](https://www.davidsilver.uk/teaching/)
- [Spinning Up in Deep RL (OpenAI)](https://spinningup.openai.com/)
- [Coursera - Reinforcement Learning Specialization](https://www.coursera.org/specializations/reinforcement-learning)

### Libros
- **"Reinforcement Learning: An Introduction" - Sutton & Barto** (La biblia del RL)
- "Deep Reinforcement Learning Hands-On" - Maxim Lapan
- "Foundations of Deep Reinforcement Learning" - Laura Graesser, Wah Loon Keng
- "Algorithms for Reinforcement Learning" - Csaba Szepesv치ri

### Papers Fundamentales
- "Playing Atari with Deep Reinforcement Learning" (DQN, 2013)
- "Human-level control through deep reinforcement learning" (Nature DQN, 2015)
- "Continuous control with deep reinforcement learning" (DDPG, 2015)
- "Asynchronous Methods for Deep Reinforcement Learning" (A3C, 2016)
- "Proximal Policy Optimization Algorithms" (PPO, 2017)
- "Mastering the game of Go with deep neural networks" (AlphaGo, 2016)
- "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (MuZero, 2019)

### Blogs y Recursos
- [OpenAI Blog - RL Section](https://openai.com/blog/tags/reinforcement-learning/)
- [DeepMind Blog](https://deepmind.com/blog)
- [Lil'Log - RL Posts](https://lilianweng.github.io/lil-log/)
- [Distill.pub - RL Articles](https://distill.pub/)

## 游 Proyectos Sugeridos

### Principiantes
1. **CartPole**: Balance de p칠ndulo invertido
2. **Mountain Car**: Carro en una colina
3. **Frozen Lake**: Grid world simple
4. **Taxi**: Navegaci칩n y pickup/dropoff

### Intermedios
5. **LunarLander**: Control continuo
6. **Atari Games**: Breakout, Pong, Space Invaders
7. **BipedalWalker**: Locomoci칩n
8. **Custom Grid Worlds**: Crear tus propios entornos

### Avanzados
9. **Multi-Agent Soccer**: Coordinaci칩n de equipos
10. **Robotic Manipulation**: Brazo rob칩tico con MuJoCo
11. **Trading Bot**: RL para trading algor칤tmico
12. **Autonomous Driving**: Simulaci칩n de conducci칩n
13. **Dota 2/StarCraft II**: RL en juegos complejos

## 游늵 Entornos Populares

### Cl치sicos
- CartPole-v1
- MountainCar-v0
- Acrobot-v1
- Pendulum-v1

### Atari
- Breakout
- Pong
- Space Invaders
- Pac-Man

### Control Continuo
- LunarLander
- BipedalWalker
- HalfCheetah (MuJoCo)
- Ant (MuJoCo)
- Humanoid (MuJoCo)

### Rob칩tica
- FetchReach
- HandManipulate
- ShadowHand

## 游눠 Best Practices

1. **Empieza simple**: Usa CartPole antes de Atari
2. **Hyperparameter tuning**: RL es muy sensible a hiperpar치metros
3. **Semilla aleatoria**: Usa m칰ltiples seeds para evaluaci칩n
4. **Monitoreo**: Tracking de recompensas, loss, entrop칤a
5. **Normalizaci칩n**: Normaliza observaciones y recompensas
6. **Debugging**: RL es dif칤cil de debuggear, empieza con implementaciones probadas
7. **Evaluation**: Separa entrenamiento de evaluaci칩n
8. **Paciencia**: RL requiere mucho tiempo de entrenamiento
9. **Baseline**: Compara con algoritmos establecidos
10. **Reproducibilidad**: Documenta seeds, hiperpar치metros, c칩digo

## 游꿢 Aplicaciones del Mundo Real

### Rob칩tica
- Manipulaci칩n de objetos
- Locomoci칩n
- Navegaci칩n aut칩noma
- Assembly tasks

### Juegos
- AlphaGo, AlphaZero
- Dota 2 (OpenAI Five)
- StarCraft II (AlphaStar)

### Finanzas
- Trading algor칤tmico
- Portfolio management
- Option pricing

### Sistemas de Recomendaci칩n
- Personalizaci칩n de contenido
- Ad placement
- News recommendation

### Recursos y Energ칤a
- Control de HVAC
- Grid optimization
- Resource allocation

### Salud
- Treatment planning
- Drug discovery
- Personalized medicine

## 游댧 Tendencias Actuales

- **Offline RL**: Aprender de datos hist칩ricos sin interacci칩n
- **Meta-RL**: Aprender a aprender
- **World Models**: Modelos generativos del entorno
- **Sim-to-Real**: Transferencia de simulaci칩n a mundo real
- **Safe RL**: RL con restricciones de seguridad
- **Multi-task RL**: Un agente, m칰ltiples tareas
- **Explainable RL**: Interpretabilidad de pol칤ticas
- **Sample Efficiency**: Reducir muestras necesarias

## 丘멆잺 Desaf칤os Comunes y Soluciones

1. **Sample Inefficiency**: Millones de pasos de entrenamiento
   - *Soluciones*: Model-based RL, curriculum learning, transfer learning, usar algoritmos off-policy (SAC, TD3)

2. **Inestabilidad**: Colapso del entrenamiento
   - *Soluciones*: Target networks, gradient clipping, normalizaci칩n de rewards, usar PPO en lugar de TRPO

3. **Sensibilidad a hiperpar치metros**: Peque침os cambios, grandes efectos
   - *Soluciones*: Grid search, Bayesian optimization, usar implementaciones validadas (Stable-Baselines3), documentar todos los hiperpar치metros

4. **Reproducibilidad**: Resultados variables entre runs
   - *Soluciones*: Fijar random seeds, m칰ltiples runs con estad칤sticas, documentar versiones de bibliotecas

5. **Credit Assignment**: Asignar cr칠dito a acciones pasadas
   - *Soluciones*: Usar m칠todos TD con n-step returns, GAE (Generalized Advantage Estimation), hindsight experience replay

6. **Exploration vs Exploitation**: Balance dif칤cil
   - *Soluciones*: 풧-greedy scheduling, entropy regularization, curiosity-driven exploration, count-based bonuses

7. **Sparse Rewards**: Se침ales de aprendizaje infrecuentes
   - *Soluciones*: Reward shaping, curriculum learning, hindsight experience replay, imitation learning

8. **Transferencia**: Modelos espec칤ficos a entornos
   - *Soluciones*: Domain randomization, sim-to-real techniques, meta-RL, progressive neural networks

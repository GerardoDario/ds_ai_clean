# Papers Fundamentales de IA

Este directorio contiene referencias a art√≠culos cient√≠ficos importantes en Inteligencia Artificial.

## üìö D√≥nde Encontrar Papers

### Plataformas Principales
- **[ArXiv](https://arxiv.org/)**: Preprints gratuitos (cs.AI, cs.LG, cs.CV, cs.CL)
- **[Papers With Code](https://paperswithcode.com/)**: Papers con implementaciones
- **[Google Scholar](https://scholar.google.com/)**: B√∫squeda acad√©mica
- **[Semantic Scholar](https://www.semanticscholar.org/)**: AI-powered search
- **[OpenReview](https://openreview.net/)**: Reviews de conferencias

### Conferencias Principales
- **NeurIPS** (Neural Information Processing Systems)
- **ICML** (International Conference on Machine Learning)
- **ICLR** (International Conference on Learning Representations)
- **CVPR** (Computer Vision and Pattern Recognition)
- **ICCV** (International Conference on Computer Vision)
- **ECCV** (European Conference on Computer Vision)
- **ACL** (Association for Computational Linguistics)
- **EMNLP** (Empirical Methods in NLP)
- **AAAI** (Association for the Advancement of AI)
- **IJCAI** (International Joint Conference on AI)

### Journals
- **JMLR** (Journal of Machine Learning Research)
- **Nature Machine Intelligence**
- **IEEE TPAMI** (Transactions on Pattern Analysis and Machine Intelligence)
- **AI Journal**

## üåü Papers Hist√≥ricos y Fundacionales

### Fundamentos de IA
- **"Computing Machinery and Intelligence" (1950)**
  - Alan Turing
  - El Test de Turing

- **"A Logical Calculus of Ideas Immanent in Nervous Activity" (1943)**
  - McCulloch & Pitts
  - Primera red neuronal artificial

### Machine Learning Cl√°sico
- **"Learning representations by back-propagating errors" (1986)**
  - Rumelhart, Hinton, Williams
  - Backpropagation

- **"A Decision-Theoretic Generalization of On-Line Learning" (1997)**
  - Freund & Schapire
  - AdaBoost

- **"Support-Vector Networks" (1995)**
  - Cortes & Vapnik
  - SVMs

## üß† Deep Learning

### Arquitecturas Fundamentales
- **"ImageNet Classification with Deep Convolutional Neural Networks" (2012)**
  - Krizhevsky, Sutskever, Hinton
  - AlexNet - Inicio del deep learning moderno
  - [Paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)

- **"Very Deep Convolutional Networks for Large-Scale Image Recognition" (2014)**
  - Simonyan & Zisserman
  - VGGNet
  - [ArXiv](https://arxiv.org/abs/1409.1556)

- **"Deep Residual Learning for Image Recognition" (2015)**
  - He et al.
  - ResNet - Skip connections
  - [ArXiv](https://arxiv.org/abs/1512.03385)

- **"Going Deeper with Convolutions" (2014)**
  - Szegedy et al.
  - Inception (GoogLeNet)
  - [ArXiv](https://arxiv.org/abs/1409.4842)

### Optimizaci√≥n y Training
- **"Batch Normalization: Accelerating Deep Network Training" (2015)**
  - Ioffe & Szegedy
  - Batch normalization
  - [ArXiv](https://arxiv.org/abs/1502.03167)

- **"Adam: A Method for Stochastic Optimization" (2014)**
  - Kingma & Ba
  - Optimizador Adam
  - [ArXiv](https://arxiv.org/abs/1412.6980)

- **"Dropout: A Simple Way to Prevent Neural Networks from Overfitting" (2014)**
  - Srivastava et al.
  - Regularizaci√≥n con dropout
  - [JMLR](https://jmlr.org/papers/v15/srivastava14a.html)

## üëÅÔ∏è Computer Vision

### Detecci√≥n de Objetos
- **"You Only Look Once: Unified, Real-Time Object Detection" (2016)**
  - Redmon et al.
  - YOLO
  - [ArXiv](https://arxiv.org/abs/1506.02640)

- **"Faster R-CNN: Towards Real-Time Object Detection" (2015)**
  - Ren et al.
  - Faster R-CNN
  - [ArXiv](https://arxiv.org/abs/1506.01497)

- **"Mask R-CNN" (2017)**
  - He et al.
  - Instance segmentation
  - [ArXiv](https://arxiv.org/abs/1703.06870)

### Segmentaci√≥n
- **"Fully Convolutional Networks for Semantic Segmentation" (2015)**
  - Long, Shelhamer, Darrell
  - FCN
  - [ArXiv](https://arxiv.org/abs/1411.4038)

- **"U-Net: Convolutional Networks for Biomedical Image Segmentation" (2015)**
  - Ronneberger et al.
  - U-Net
  - [ArXiv](https://arxiv.org/abs/1505.04597)

### Generative Models
- **"Generative Adversarial Networks" (2014)**
  - Goodfellow et al.
  - GANs
  - [ArXiv](https://arxiv.org/abs/1406.2661)

- **"Auto-Encoding Variational Bayes" (2013)**
  - Kingma & Welling
  - VAE
  - [ArXiv](https://arxiv.org/abs/1312.6114)

- **"Denoising Diffusion Probabilistic Models" (2020)**
  - Ho et al.
  - Diffusion models
  - [ArXiv](https://arxiv.org/abs/2006.11239)

## üìù Natural Language Processing

### Transformers y Attention
- **"Attention Is All You Need" (2017)**
  - Vaswani et al.
  - Transformer architecture
  - [ArXiv](https://arxiv.org/abs/1706.03762)

- **"Neural Machine Translation by Jointly Learning to Align and Translate" (2014)**
  - Bahdanau et al.
  - Attention mechanism
  - [ArXiv](https://arxiv.org/abs/1409.0473)

### Language Models
- **"BERT: Pre-training of Deep Bidirectional Transformers" (2018)**
  - Devlin et al.
  - BERT
  - [ArXiv](https://arxiv.org/abs/1810.04805)

- **"Language Models are Few-Shot Learners" (2020)**
  - Brown et al.
  - GPT-3
  - [ArXiv](https://arxiv.org/abs/2005.14165)

- **"Improving Language Understanding by Generative Pre-Training" (2018)**
  - Radford et al.
  - GPT
  - [OpenAI](https://openai.com/research/language-unsupervised)

- **"RoBERTa: A Robustly Optimized BERT Pretraining Approach" (2019)**
  - Liu et al.
  - RoBERTa
  - [ArXiv](https://arxiv.org/abs/1907.11692)

- **"ELECTRA: Pre-training Text Encoders as Discriminators" (2020)**
  - Clark et al.
  - ELECTRA
  - [ArXiv](https://arxiv.org/abs/2003.10555)

### Word Embeddings
- **"Efficient Estimation of Word Representations in Vector Space" (2013)**
  - Mikolov et al.
  - Word2Vec
  - [ArXiv](https://arxiv.org/abs/1301.3781)

- **"GloVe: Global Vectors for Word Representation" (2014)**
  - Pennington et al.
  - GloVe
  - [Website](https://nlp.stanford.edu/projects/glove/)

## üéÆ Reinforcement Learning

### Fundamentales
- **"Playing Atari with Deep Reinforcement Learning" (2013)**
  - Mnih et al. (DeepMind)
  - DQN
  - [ArXiv](https://arxiv.org/abs/1312.5602)

- **"Human-level control through deep reinforcement learning" (2015)**
  - Mnih et al.
  - DQN en Nature
  - [Nature](https://www.nature.com/articles/nature14236)

- **"Asynchronous Methods for Deep Reinforcement Learning" (2016)**
  - Mnih et al.
  - A3C
  - [ArXiv](https://arxiv.org/abs/1602.01783)

### Policy Gradient
- **"Proximal Policy Optimization Algorithms" (2017)**
  - Schulman et al.
  - PPO
  - [ArXiv](https://arxiv.org/abs/1707.06347)

- **"Trust Region Policy Optimization" (2015)**
  - Schulman et al.
  - TRPO
  - [ArXiv](https://arxiv.org/abs/1502.05477)

- **"Continuous control with deep reinforcement learning" (2015)**
  - Lillicrap et al.
  - DDPG
  - [ArXiv](https://arxiv.org/abs/1509.02971)

### Game Playing
- **"Mastering the game of Go with deep neural networks" (2016)**
  - Silver et al. (DeepMind)
  - AlphaGo
  - [Nature](https://www.nature.com/articles/nature16961)

- **"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (2019)**
  - Schrittwieser et al.
  - MuZero
  - [ArXiv](https://arxiv.org/abs/1911.08265)

## üîÑ Transfer Learning

- **"A Survey on Transfer Learning" (2010)**
  - Pan & Yang
  - Survey comprehensivo
  - [IEEE](https://ieeexplore.ieee.org/document/5288526)

- **"How transferable are features in deep neural networks?" (2014)**
  - Yosinski et al.
  - [ArXiv](https://arxiv.org/abs/1411.1792)

## üéØ Vision Transformers

- **"An Image is Worth 16x16 Words: Transformers for Image Recognition" (2020)**
  - Dosovitskiy et al.
  - Vision Transformer (ViT)
  - [ArXiv](https://arxiv.org/abs/2010.11929)

- **"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" (2021)**
  - Liu et al.
  - Swin Transformer
  - [ArXiv](https://arxiv.org/abs/2103.14030)

## üåê Multimodal Learning

- **"Learning Transferable Visual Models From Natural Language Supervision" (2021)**
  - Radford et al.
  - CLIP
  - [ArXiv](https://arxiv.org/abs/2103.00020)

- **"Flamingo: a Visual Language Model for Few-Shot Learning" (2022)**
  - Alayrac et al.
  - Flamingo
  - [ArXiv](https://arxiv.org/abs/2204.14198)

## ‚öñÔ∏è IA √âtica y Fairness

- **"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings" (2016)**
  - Bolukbasi et al.
  - Sesgo en embeddings
  - [ArXiv](https://arxiv.org/abs/1607.06520)

- **"Fairness Through Awareness" (2012)**
  - Dwork et al.
  - Fairness en ML
  - [Paper](https://arxiv.org/abs/1104.3913)

- **"Model Cards for Model Reporting" (2019)**
  - Mitchell et al.
  - Documentaci√≥n de modelos
  - [ArXiv](https://arxiv.org/abs/1810.03993)

## üî¨ Explicabilidad (XAI)

- **"Why Should I Trust You?" Explaining Predictions of Any Classifier" (2016)**
  - Ribeiro et al.
  - LIME
  - [ArXiv](https://arxiv.org/abs/1602.04938)

- **"A Unified Approach to Interpreting Model Predictions" (2017)**
  - Lundberg & Lee
  - SHAP
  - [ArXiv](https://arxiv.org/abs/1705.07874)

## üìö Surveys y Reviews

### General AI/ML
- **"Deep Learning" (2015)**
  - LeCun, Bengio, Hinton
  - Nature review
  - [Nature](https://www.nature.com/articles/nature14539)

### Computer Vision
- **"Object Detection in 20 Years: A Survey" (2019)**
  - Zou et al.
  - [ArXiv](https://arxiv.org/abs/1905.05055)

### NLP
- **"Recent Advances in Deep Learning for Natural Language Processing" (2018)**
  - Young et al.
  - [ArXiv](https://arxiv.org/abs/1708.02709)

- **"Pre-trained Models for Natural Language Processing: A Survey" (2020)**
  - Qiu et al.
  - [ArXiv](https://arxiv.org/abs/2003.08271)

### Reinforcement Learning
- **"Deep Reinforcement Learning: An Overview" (2018)**
  - Li
  - [ArXiv](https://arxiv.org/abs/1701.07274)

## üí° C√≥mo Leer Papers de IA

### Estrategia de Lectura
1. **Primera pasada (5-10 min)**
   - Lee t√≠tulo, abstract, conclusiones
   - Mira figuras y tablas
   - Decide si vale la pena leer completo

2. **Segunda pasada (1 hora)**
   - Lee introducci√≥n y related work
   - Entiende metodolog√≠a a alto nivel
   - Ignora detalles matem√°ticos complejos
   - Anota preguntas

3. **Tercera pasada (4-5 horas)**
   - Lee en profundidad
   - Entiende cada ecuaci√≥n
   - Identifica fortalezas y debilidades
   - Considera reimplementaci√≥n

### Tips
- **Lee relacionados primero**: Papers citados pueden dar contexto
- **Toma notas**: Resume en tus propias palabras
- **Implementa**: La mejor forma de entender
- **Discute**: Habla con otros sobre el paper
- **Lee c√≥digo**: Si est√° disponible

## üîç Herramientas para Papers

### Gesti√≥n de Referencias
- **Zotero**: Gestor de referencias gratuito
- **Mendeley**: Gestor y red social
- **Papers**: Gestor para Mac
- **Paperpile**: Basado en web

### Lectura y Anotaci√≥n
- **Connected Papers**: Visualiza relaciones entre papers
- **Semantic Scholar**: B√∫squeda inteligente
- **arXiv Vanity**: Renderiza papers en HTML
- **Scholarcy**: Summarizaci√≥n autom√°tica
- **Notion**: Notas y organizaci√≥n

### Seguimiento
- **Twitter acad√©mico**: Muchos autores comparten trabajos
- **ArXiv Sanity**: Curated feed de arXiv
- **Papers With Code Newsletter**: Resumen semanal
- **Reddit r/MachineLearning**: Discusi√≥n de papers

## üìñ Recursos para Aprender a Leer Papers

- [How to Read a Paper - S. Keshav](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf)
- [Reading Research Papers - Andrew Ng](https://www.youtube.com/watch?v=733m6qBH-jI)
- [Papers We Love](https://paperswelove.org/)

## üéì Papers por Nivel

### Principiantes (M√°s Accesibles)
- AlexNet paper
- ResNet paper
- YOLO paper
- Word2Vec paper

### Intermedios
- Attention Is All You Need (Transformer)
- BERT paper
- Generative Adversarial Networks

### Avanzados (M√°s T√©cnicos)
- Variational Autoencoders
- Neural ODEs
- Meta-learning papers
- Theoretical ML papers

## üèÜ Papers M√°s Citados (Top 10)

1. **ImageNet Classification** (AlexNet) - 100K+ citas
2. **Attention Is All You Need** - 80K+ citas
3. **Deep Residual Learning** (ResNet) - 120K+ citas
4. **Generative Adversarial Networks** - 50K+ citas
5. **BERT** - 70K+ citas
6. **Adam Optimizer** - 90K+ citas
7. **Batch Normalization** - 60K+ citas
8. **Dropout** - 40K+ citas
9. **Playing Atari with Deep RL** (DQN) - 20K+ citas
10. **VGG** - 80K+ citas

## ‚≠ê Papers Recomendados para Empezar

Si eres nuevo en leer papers de IA, empieza con estos:
1. ImageNet Classification (AlexNet)
2. Deep Residual Learning (ResNet)
3. Attention Is All You Need (Transformer)
4. BERT
5. Playing Atari with Deep RL (DQN)

Estos son relativamente accesibles, muy influyentes y bien escritos.
